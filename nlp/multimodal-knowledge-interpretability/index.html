

<!DOCTYPE html>
<html lang="zh-TW" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.png">
  <link rel="icon" href="/img/avatar.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="本篇介紹三篇多模態知識可解釋性相關新近工作，這些工作做了多模態模型學習到的知識的可視化或分析性實驗。">
  <meta name="author" content="Enatsu">
  <meta name="keywords" content="">
  
  <title>多模態知識可解釋性 Multimodal Knowledge Interpretability - Enatsu&#39;s Site</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.6.0/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/custom.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Enatsu's Site</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首頁
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                歸檔
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分類
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                標籤
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                關於
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友站
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
    <div class="text-center navbar-convert-div"><button id="btn-simplify" type="button" class="navbar-convert">繁</button></div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/images/cone-effect.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="多模態知識可解釋性 Multimodal Knowledge Interpretability">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-04-17 08:00" pubdate>
        2022年4月17日 早上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      凡 1.8k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      7
       分鐘
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">多模態知識可解釋性 Multimodal Knowledge Interpretability</h1>
            
            <div class="markdown-body">
              <p>本篇介紹三篇<strong>多模態知識可解釋性</strong>相關新近工作，這些工作做了多模態模型學習到的知識的可視化或分析性實驗。</p>
<span id="more"></span>

<h2 id="Finding-Structural-Knowledge-in-Multimodal-BERT"><a href="#Finding-Structural-Knowledge-in-Multimodal-BERT" class="headerlink" title="Finding Structural Knowledge in Multimodal-BERT"></a>Finding Structural Knowledge in Multimodal-BERT</h2><p><em>ACL2022, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.09306">arXiv:2203.09306</a></em></p>
<p>探究多模態模型嵌入（每層編碼器的輸入。最底層即輸入的embedding，其他爲前一層輸出的隱狀態）表示蘊含的結構知識。</p>
<p><strong>研究問題</strong></p>
<ul>
<li>RQ 1: 多模態BERT的文本嵌入相比於文本BERT是否保留結構知識？<ul>
<li>Sub-RQ 1.1：文本-圖像聯合訓練是否有影響？</li>
</ul>
</li>
<li>RQ 2: 多模態BERT的視覺嵌入是否學習到如何編碼視覺<strong>場景樹（scene tree）</strong>？</li>
</ul>
<p><strong>主要貢獻</strong></p>
<ul>
<li>根據文本依存樹，爲圖像定義場景樹</li>
<li>利用<strong>結構探針（structural probe）</strong>，輸入文本/圖像結點的模型嵌入，預測文本依存樹或圖像場景樹結點深度和距離</li>
</ul>
<h3 id="構建場景樹"><a href="#構建場景樹" class="headerlink" title="構建場景樹"></a>構建場景樹</h3><p><img src="/images/multi-probe-scene-tree.png" srcset="/img/loading.gif" lazyload alt="Scene tree"></p>
<p>輸入</p>
<ul>
<li>文本依存樹$T_t=\{ E_t, V_t \}$</li>
<li>視覺短語（phrase，含圖像區域和對應文本）集合$P$</li>
<li>圖像$I$</li>
</ul>
<p>輸出</p>
<ul>
<li>場景樹$T_s=\{ E_s, V_s \}$ (根結點$v_{s,0}$爲$I$) </li>
<li>結點之間的距離矩陣$\boldsymbol{D} \in \mathbb{D}^{n\times n}$</li>
<li>結點深度向量$\boldsymbol{d} \in \mathbb{D}^{n}$。</li>
</ul>
<h3 id="結構探針"><a href="#結構探針" class="headerlink" title="結構探針"></a>結構探針</h3><p>輸入：文本/圖像結點的嵌入<br>輸出：預測的文本依存樹/圖像場景樹距離矩陣、深度向量</p>
<ul>
<li>距離探針 $\boldsymbol{D}_{ij}=(\boldsymbol{B}(\boldsymbol{h}_i^l-\boldsymbol{h}_j^l))^\top(\boldsymbol{B}(\boldsymbol{h}_i^l-\boldsymbol{h}_j^l))$</li>
<li>深度探針 $\boldsymbol{d}_{i}=||\boldsymbol{h}_i||_{\boldsymbol{B}}^2=(\boldsymbol{B}\boldsymbol{h}_i^l)^\top(\boldsymbol{B}\boldsymbol{h}_i^l)$</li>
</ul>
<h3 id="實驗"><a href="#實驗" class="headerlink" title="實驗"></a>實驗</h3><p>純文本：Penn Treebank (PTB3)<br>圖文：Flickr30k</p>
<p>模型</p>
<ul>
<li>文本：BERT</li>
<li>單流：UNITER</li>
<li>雙流：ViLBERT</li>
</ul>
<p>基線：未作微調的zero-shot文本/圖像嵌入；原始RCNN特徵。</p>
<h3 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h3><p><img src="/images/multi-probe-ptb3.png" srcset="/img/loading.gif" lazyload alt="Text probe on PTB3"></p>
<p><em>模型中間層嵌入蘊含更多結構知識。</em></p>
<p><strong>RQ1</strong>結論：在PTB3數據集上，多模態模型保留的文本結構知識不及BERT。</p>
<hr>
<p><img src="/images/multi-probe-flickr1.png" srcset="/img/loading.gif" lazyload alt="Text probe on Flickr30k"></p>
<p><img src="/images/multi-probe-flickr2.png" srcset="/img/loading.gif" lazyload alt="Text probe on Flickr30k w/o visual embeddings"></p>
<p><strong>Sub-RQ1</strong>結論：在Flickr30k數據集上，效果和BERT差不多，但並非視覺嵌入參與訓練的貢獻。</p>
<p><em>（合理懷疑：Flickr30k畢竟是圖文數據；PTB3可能跨領域了）</em></p>
<hr>
<p><img src="/images/multi-probe-flickr3.png" srcset="/img/loading.gif" lazyload alt="Visual probe on Flickr30k"></p>
<p><strong>RQ2</strong>結論：視覺嵌入構建場景樹的效果不如利用原始RCNN特徵。</p>
<h2 id="VL-InterpreT-An-Interactive-Visualization-Tool-for-Interpreting-Vision-Language-Transformers"><a href="#VL-InterpreT-An-Interactive-Visualization-Tool-for-Interpreting-Vision-Language-Transformers" class="headerlink" title="VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers"></a>VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers</h2><p><em><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.17247">arXiv:2203.17247</a></em></p>
<p><strong>（單流）</strong>多模態Transformer注意力和表示的<strong>可視化工具</strong></p>
<ul>
<li><strong>注意力</strong>：考慮每層的注意力頭，以及跨模態/模態內注意力，構成大小爲$(N_{layers},N_{heads},L_{v}+L_{l},L_{v}+L_{l})$的注意力矩陣。對各層各注意力頭注意力權重、各token注意力權重可繪製熱力圖。</li>
<li><strong>隱狀態表示</strong>：t-SNE降維到2維，可視化。</li>
</ul>
<p>對KD-VLP模型在兩個視覺問答任務VCR（Visual Commonsense Reasoning，視覺常識推理）和WebQA上做可視化分析（略）。</p>
<p><img src="/images/vl-interpret.png" srcset="/img/loading.gif" lazyload alt="VL-InterpreT"></p>
<h2 id="Mind-the-Gap-Understanding-the-Modality-Gap-in-Multi-modal-Contrastive-Representation-Learning"><a href="#Mind-the-Gap-Understanding-the-Modality-Gap-in-Multi-modal-Contrastive-Representation-Learning" class="headerlink" title="Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning"></a>Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</h2><p><em><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02053">arXiv:2203.02053</a></em></p>
<p>探究多模態表示空間的間隔。如CLIP一類<strong>雙流</strong>模型的模態表示間隔是由於不同的<strong>模型初始化</strong>和對比學習<strong>優化</strong>所造成；非線性激活函數可造成圓錐效應；改變模態間隔，可改善下游zero-shot分類性能並減少種族偏見。</p>
<p>對模態間隔現象提供三个方面的解釋：</p>
<ul>
<li>深度神經結構的歸納偏置（inductive bias）產生<strong>圓錐效應（cone effect）</strong>：無論是否預訓練過，模型的向量空間均被限制在狹窄的圓錐中。</li>
<li>不同的隨機初始化產生不同的嵌入向量錐。</li>
<li>對比學習目標函數保留了這個間隔。</li>
</ul>
<p>理論分析表明，每個神經網絡層有大概率縮小了嵌入向量之間的角度，在更深的網絡結構下創建更窄的圓錐。</p>
<h3 id="模態間隔"><a href="#模態間隔" class="headerlink" title="模態間隔"></a>模態間隔</h3><p><img src="/images/modality-gap.png" srcset="/img/loading.gif" lazyload alt="Modality gap"></p>
<ul>
<li>各種模態的（雙流）模型</li>
<li>隨機權重仍有此現象</li>
</ul>
<h3 id="圓錐效應致模態間隔"><a href="#圓錐效應致模態間隔" class="headerlink" title="圓錐效應致模態間隔"></a>圓錐效應致模態間隔</h3><p><img src="/images/cone-effect.png" srcset="/img/loading.gif" lazyload alt="Cone effect"></p>
<h4 id="嵌入的狹窄圓錐"><a href="#嵌入的狹窄圓錐" class="headerlink" title="嵌入的狹窄圓錐"></a>嵌入的狹窄圓錐</h4><p>如上圖(a)：</p>
<ul>
<li>平均/最小相似度均爲正數</li>
<li>隨機權重的模型也存在該效應，甚於預訓練過的</li>
<li>隨機噪聲（視覺：正態分佈/文本：隨機數）+隨機權重的模型也存在該效應</li>
</ul>
<p><strong>結論</strong>：圓錐效應反映深度模型普遍的歸納偏置；先前工作的解釋稱「不平衡的詞頻分佈使優化有偏置」，不夠準確。</p>
<h4 id="非線性激活函數的影響"><a href="#非線性激活函數的影響" class="headerlink" title="非線性激活函數的影響"></a>非線性激活函數的影響</h4><p>如上圖(b)，考察各種MLP，無激活函數MLP隨層數增加圓錐效應不明顯，反之有激活函數的多層MLP餘弦相似度迅速上升（ReLU輸出非負，故餘弦相似度上升最快）。</p>
<p><strong>結論</strong>：非線性激活函數對圓錐效應有重要影響。</p>
<p><em>層歸一化的影響待探究。</em></p>
<h4 id="不同隨機初始化"><a href="#不同隨機初始化" class="headerlink" title="不同隨機初始化"></a>不同隨機初始化</h4><p>如上圖(c)。</p>
<p><strong>結論</strong>：不同隨機初始化產生不同的圓錐。</p>
<h3 id="理論分析"><a href="#理論分析" class="headerlink" title="理論分析"></a>理論分析</h3><p>對上述現象的數學證明（略）</p>
<ul>
<li>餘弦相似度隨層數加深逐漸增加</li>
<li>中間層輸出的方差主要源於模型的不同隨機初始化</li>
</ul>
<h3 id="CLIP一類對比學習保留模態間隔"><a href="#CLIP一類對比學習保留模態間隔" class="headerlink" title="CLIP一類對比學習保留模態間隔"></a>CLIP一類對比學習保留模態間隔</h3><p>分析CLIP的InfoNCE中溫度$\tau$的影響。</p>
<p>定義<strong>模態間隔</strong>爲兩個模態嵌入表示中心點的歐式距離</p>
<p>$$\vec{\Delta}_{\mathrm{gap}}=\frac{1}{n}\sum_{i=1}^n\mathbf{x}_i - \frac{1}{n}\sum_{i=1}^n\mathbf{y}_i$$</p>
<p>於是可調整根據間隔方向調整模態差距</p>
<p>$$<br>\begin{array} \\<br>\mathbf{x}_i^\mathrm{shift}=\mathrm{Normalize}(\mathbf{x}_i - \lambda \vec{\Delta}_{\mathrm{gap}}) \\<br>\mathbf{y}_i^\mathrm{shift}=\mathrm{Normalize}(\mathbf{y}_i + \lambda \vec{\Delta}_{\mathrm{gap}})<br>\end{array}<br>$$</p>
<p>CLIP模型學習到的溫度$\tau$=0.01，在MSCOCO驗證集上計算得到的間隔$||\vec{\Delta}_{\mathrm{gap}}||$=0.82。</p>
<p>由下圖可知，$||\vec{\Delta}_{\mathrm{gap}}||$=0.82時在驗證集得到的損失最小；溫度爲1時，傾向於縮小間隔。</p>
<p><img src="/images/cl-preserves-gap.png" srcset="/img/loading.gif" lazyload alt="Contrastive learning preserves modality gap"></p>
<p>進一步微調實驗證實，較大的溫度（$\tau\in \{ \frac{1}{10}, 1 \}$）縮小了間隔。</p>
<p><img src="/images/fine-tune-temperature.png" srcset="/img/loading.gif" lazyload alt="Reduce the gap by fine-tuning with high temperature"></p>
<p>3維球坐標上存在部分不匹配圖文對（如圖中$T_1$和$I_0$更近）的模擬實驗也證明：設定較小溫度時，只有在球坐標上保持一定的角度差距，纔能使損失最低；設定較大的溫度則無此現象。額外的模擬實驗說明，如果圖文完全匹配則無此現象。</p>
<p align="center">
  <img src="/images/mismatch-simulation.png" srcset="/img/loading.gif" lazyload alt="with and without mismatched data" width="50%" height="50%">
</p>

<h4 id="初始化和優化的共同作用"><a href="#初始化和優化的共同作用" class="headerlink" title="初始化和優化的共同作用"></a>初始化和優化的共同作用</h4><p>另有CLIP訓練實驗，一採用隨機初始化，模態間隔 $||\vec{\Delta}_{\mathrm{gap}}|| = 1.1891 \pm 0.0017$；一將文本模態通過線性變換拉近至$||\vec{\Delta}_{\mathrm{gap}}|| = 0.0388 ± 0.0351$。訓練後：1.1891±0.0017變1.2991±0.0389，0.0388±0.0351變0.7457±0.0633。</p>
<p><strong>結論</strong>：初始化和優化共同導致模態間隔。即使初始化幾乎無間隔，對比學習損失函數的優化仍然造成間隔，儘管間隔較小。</p>
<h4 id="實驗-1"><a href="#實驗-1" class="headerlink" title="實驗"></a>實驗</h4><p><img src="/images/modal-gap-exp.png" srcset="/img/loading.gif" lazyload alt="Experiments"></p>
<p>適當調整模態間隔，有助於調整模型zero-shot性能，可減少模型在人臉識別任務上的種族偏見。</p>
<p><em>間隔太小：犯罪相關偏見；間隔太大：非人類偏見。</em></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/nlp/">NLP</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/NLP/">NLP</a>
                    
                      <a class="hover-with-bg" href="/tags/multimodal/">多模態</a>
                    
                      <a class="hover-with-bg" href="/tags/CV/">CV</a>
                    
                      <a class="hover-with-bg" href="/tags/contrastive-learning/">對比學習</a>
                    
                      <a class="hover-with-bg" href="/tags/interpretability/">可解釋性</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本站所有文章除特別聲明外，均採用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 協議</a> ，轉載請註明出處！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/nlp/TransformerBasedNLGModels/">
                        <span class="hidden-mobile">基于Transformer的自然语言生成模型</span>
                        <span class="visible-mobile">後篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目錄</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">檢索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">關鍵詞</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <div> <a target="_blank" rel="noopener" href='https://blog.enatsu.top'>Enatsu's Site</a> <span> © 2019-2021</span> </div> <div> <span>Powered by</span> <a href='https://hexo.io' target='_blank' rel='nofollow noopener'><span>Hexo</span></a> <i class='iconfont icon-love'></i> <a href='https://github.com/fluid-dev/hexo-theme-fluid' target='_blank' rel='nofollow noopener'> <span>Fluid</span></a> </div> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            <i class="iconfont icon-eye"></i> 
            <span id="busuanzi_value_site_pv"></span>
            
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            <i class="iconfont icon-users"></i> 
            <span id="busuanzi_value_site_uv"></span>
            
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.staticfile.org/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.6.0/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- OpenCC -->
<script  src="/opencc-js/1.0.3/data.min.js" ></script>
<script  src="/opencc-js/1.0.3/data.t2cn.min.js" ></script>
<script  src="/opencc-js/1.0.3/bundle-browser.min.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.staticfile.org/tocbot/4.12.0/tocbot.min.js" ></script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.8/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.bootcdn.net/ajax/libs/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.1.4/es5/tex-svg.js" ></script>

  










  
<script src="/js/custom.js"></script>
<script src="/js/jquery.cookie.js"></script>



<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
