

<!DOCTYPE html>
<html lang="zh-TW" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.png">
  <link rel="icon" href="/img/avatar.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="一些 Transformer 学习笔记。">
  <meta name="author" content="Enatsu">
  <meta name="keywords" content="">
  
  <title>基于Transformer的自然语言生成模型 - Enatsu&#39;s Site</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.6.0/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/custom.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Enatsu's Site</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首頁
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                歸檔
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分類
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                標籤
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                關於
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友站
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
    <div class="text-center navbar-convert-div"><button id="btn-simplify" type="button" class="navbar-convert">繁</button></div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/images/Transformer.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="基于Transformer的自然语言生成模型">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-08-04 08:00" pubdate>
        2021年8月4日 早上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      凡 958 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      4
       分鐘
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">基于Transformer的自然语言生成模型</h1>
            
            <div class="markdown-body">
              <p>一些 Transformer 学习笔记。</p>
<span id="more"></span>
<p>以下说法多为特例，勿视为通用情况。</p>
<h1>多头注意力机制</h1>
<ul>
<li>查询序列：${\pmb Q}=[{\pmb q}_1;{\pmb q}_2;…;{\pmb q}_N]^\top \in {\Bbb R}_{N\times d}$，用于检索上下文信息。</li>
<li>键值序列：查询序列关注的对象，实际包含两个序列，构成键值对$({\pmb K},{\pmb V})$。
<ul>
<li>键矩阵${\pmb K}=[{\pmb k}_1;{\pmb k}_2;…;{\pmb k}_M]^\top \in {\Bbb R}_{M\times d}$</li>
<li>值矩阵${\pmb V}=[{\pmb v}_1;{\pmb v}_2;…;{\pmb v}_M]^\top \in {\Bbb R}_{M\times d}$</li>
</ul>
</li>
</ul>
<p><strong>点乘注意力</strong>（Dot-Product Attention）：$\beta_{i,j}={\pmb q}_i\cdot{\pmb k}_j$</p>
<p>注意力分数归一化：$\alpha_{i,j}={\rm softmax}({\pmb q}_i,{\pmb k}_j)$</p>
<p>第$i$个位置的上下文向量：${\rm Dot\_Attention}({\pmb q}_i,{\pmb K},{\pmb V})=\sum_{j=1}^M \alpha_{i,j} {\pmb v}_j$</p>
<p>综上：$${\rm Attention}({\pmb Q},{\pmb K},{\pmb V})={\rm softmax}({\pmb Q}{\pmb K}^\top){\pmb V} \in {\Bbb R}_{N\times d}$$</p>
<p><strong>缩放点乘注意力</strong>（Scaled Dot-Product Attention）：随着隐向量维度$d$增大，${\pmb q}_i\cdot {\pmb k}_j$的方差也逐渐增大：</p>
<blockquote>
<p>设${\pmb x}, {\pmb y}$为$n$维独立向量且${\rm var}({\pmb x})={\rm var}({\pmb y})=1$，${\rm E}({\pmb x})={\rm E}({\pmb y})=0$，则${\pmb x}\cdot{\pmb y}=\sum_{i=1}^n x_i y_i$的均值为0，方差为$n$。</p>
</blockquote>
<p>为了解决归一化后注意力分布尖锐导致梯度消失的问题，提出缩放点乘注意力：$${\rm Attention}({\pmb Q},{\pmb K},{\pmb V})={\rm softmax}( \frac{ {\pmb Q}{\pmb K}^\top}{\sqrt{d}} ){\pmb V}$$</p>
<p><strong>多头注意力</strong>（Multi-Head Attention）：促使注意力能关注序列不同位置。将${\pmb Q},{\pmb K},{\pmb V}$通过$h$组映射矩阵${\pmb W}_i^q,{\pmb W}_i^k,{\pmb W}_i^v \in {\Bbb R}^{d\times d/h}$映射到$h$个子空间，进行注意力运算，再拼接，并通过输出映射矩阵${\pmb W}^O\in {\Bbb {R}}^{d\times d}$映射回原始空间${\Bbb{R}}^{N\times d}$：</p>
<p>$$<br>
\begin{array}<br>
\\<br>
{\pmb H}_i={\rm Attention}({\pmb Q}{\pmb W}_i^q,{\pmb K}{\pmb W}_i^k,{\pmb V}{\pmb W}_i^v)\\<br>
{\rm MHA}({\pmb Q},{\pmb K},{\pmb V})=({\pmb H}_1\oplus{\pmb H}_2\oplus…\oplus{\pmb H}_h){\pmb W}^O<br>
\end{array}<br>
$$</p>
<p><img src="/images/ScaledDotProductAttention_MultiHeadAttention.png" srcset="/img/loading.gif" lazyload alt="两种注意力"></p>
<h1>Transformer基本单元</h1>
<p>Transformer基本单元主要由两部分构成：</p>
<ul>
<li>第一部分由多头自注意力机制、残差连接与层归一化组成。</li>
<li>第二部分由前馈全连接网络、残差连接与层归一化组成。</li>
</ul>
<p>形式化表示为：</p>
<p>$$\begin{array}<br>
\\<br>
{\pmb H}_{(1)}^l = {\rm{LayerNorm}}({\rm{MHA}}({\pmb H}^l,{\pmb H}^l,{\pmb H}^l)+{\pmb H}^l)\\<br>
{\pmb H}^{l+1} = {\rm{LayerNorm}}({\rm{FeedForward}}({\pmb H}_{(1)}^l)+{\pmb H}_{(1)}^l)<br>
\end{array}$$</p>
<p>下面具体介绍各个模块。</p>
<ul>
<li><strong>多头自注意力</strong>（Multi-Head Self Attention）机制：输入序列${\pmb{H}}$同时作为${\pmb{K}}$, ${\pmb{Q}}$, ${\pmb{V}}$，通过计算序列对其自身各个位置的注意力分布来建模序列的依赖关系，从而获得输入序列的上下文表示。</li>
<li><strong>残差连接</strong>（Residual Connection）：一般附加在另一模块之上，将该模块的输入${\pmb x}$与输出$f({\pmb {x}})$相加。此方法能够在模型层数较多时，将模型底层信息转递到模型的高层，一定程度上缓解梯度消失的问题。</li>
<li><strong>层归一化</strong>（Layer Normalization）：对输入的向量进行归一化操作。设输入向量${\pmb x}=[x_1,x_2,…,x_d]$的维度为$d$，利用其均值$\mu$和方差$\sigma$作归一化，使得特征的方差在不同深度的模块中保持一定的范围，让梯度更加稳定：$${\rm {LayerNorm}}({\pmb x})=\frac{\pmb g}{\sigma}\otimes({\pmb x}-\mu)+{\pmb b}$$其中${\pmb g}$和${\pmb b}$为可学习的权重向量。</li>
<li><strong>前馈网络</strong>（Feed-Forward Network）：$${\rm{FeedForward}}({\pmb x})={\rm ReLU}(x{\pmb W}_1+b_1){\pmb W}_2+b_2$$</li>
</ul>
<h1>Transformer结构</h1>
<p align="center">
  <img src="/images/Transformer.jpg" srcset="/img/loading.gif" lazyload width="300px" alt="Transformer结构">
</p>
<h1>参考资料</h1>
<p>[1] 黄民烈等. 《现代自然语言生成》. 电子工业出版社, 2021.<br>
[2] <a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/nlp/">NLP</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/nlp/">NLP</a>
                    
                      <a class="hover-with-bg" href="/tags/transformer/">Transformer</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本站所有文章除特別聲明外，均採用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 協議</a> ，轉載請註明出處！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/nlp/MultimodalKnowledgeInterpretability/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">多模態知識可解釋性 Multimodal Knowledge Interpretability</span>
                        <span class="visible-mobile">前篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/tex/MathSupport/">
                        <span class="hidden-mobile">MathJax数学符号支持</span>
                        <span class="visible-mobile">後篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目錄</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">檢索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">關鍵詞</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <div> <a target="_blank" rel="noopener" href='https://blog.enatsu.top'>Enatsu's Site</a> <span> © 2019-</span><span id='this_year'></span> </div> <div> <span>Powered by</span> <a href='https://hexo.io' target='_blank' rel='nofollow noopener'><span>Hexo</span></a> <i class='iconfont icon-love'></i> <a href='https://github.com/fluid-dev/hexo-theme-fluid' target='_blank' rel='nofollow noopener'> <span>Fluid</span></a> </div> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            <i class="iconfont icon-eye"></i> 
            <span id="busuanzi_value_site_pv"></span>
            
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            <i class="iconfont icon-users"></i> 
            <span id="busuanzi_value_site_uv"></span>
            
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.staticfile.org/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.6.0/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- OpenCC -->
<script  src="/opencc-js/1.0.3/data.min.js" ></script>
<script  src="/opencc-js/1.0.3/data.t2cn.min.js" ></script>
<script  src="/opencc-js/1.0.3/bundle-browser.min.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.staticfile.org/tocbot/4.12.0/tocbot.min.js" ></script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.8/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.bootcdn.net/ajax/libs/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.1.4/es5/tex-svg.js" ></script>

  










  
<script src="/js/custom.js"></script>
<script src="/js/jquery.cookie.js"></script>



<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
